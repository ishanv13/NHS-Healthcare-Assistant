# -*- coding: utf-8 -*-
"""NHS Healthcare Assistant (G-12).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I57qZscv3VRA9KuWhAwiUGefwbp82y3g

# Introduction

Despite the widespread adoption of Electronic Health Records (EHRs), many systems remain fragmented, difficult to navigate, and burdensome for clinicians and patients alike. Doctors spend excessive time sifting through data, while patients often struggle to understand their own medical information. This project presents **NHS Healthcare Assistant**, a Retrieval-Augmented Generation (RAG) assistant designed to solve both problems. Built with LangChain, ChromaDB, and Gemini LLM, the system ingests and indexes medical records, enabling real-time, dual-format responses: clinical summaries for providers and layperson explanations for patients. By streamlining documentation and enhancing patient comprehension, this AI assistant bridges the communication gap in healthcare—offering a scalable, cost-efficient, and patient-centered alternative to traditional EHR workflows.

# Problem Statement

Modern healthcare is overwhelmed by data—from lab results to specialist notes—often scattered across fragmented systems. This overload makes it hard for clinicians to extract key insights, leading to burnout and risk of error. Meanwhile, patients receive technical reports they can’t understand, leaving them confused and disengaged. With limited time on both sides, there's a growing gap between data and understanding. A solution is urgently needed to organize, summarize, and translate medical information—making it easier for doctors to decide and for patients to understand.

# Why Rag?

Retrieval-Augmented Generation (RAG) combines document search with answer generation, unlike standard chatbots that rely only on pre-trained knowledge. It’s ideal for healthcare, where accuracy and context matter.

Our RAG-based assistant processes medical records into searchable vectors, then uses a model like Gemini to answer questions.

#NHS Healthcare Assistant

##Load Packages

In this step, we install the core Python packages needed for the project:



**langchain:** A framework designed for building applications powered by language models, supporting document loading, text splitting, and retrieval workflows.

**pypdf:** A tool to read and manipulate PDF files, essential for extracting patient record contents.

**sentence-transformers:** Provides pre-trained models for converting text into vector embeddings, which are crucial for semantic search.

**ctransformers:** Enables integration with various language models in a performant and lightweight manner.

**chromadb:** A vector database used to store and retrieve document embeddings efficiently.


Together, these packages provide all the functionality required to ingest, process, embed, and retrieve medical documents in our pipeline.
"""

!pip install langchain pypdf sentence-transformers ctransformers chromadb -q

"""##Data Loading & Processing

In this section, we prepare the document workspace by creating a docs directory and fetching PDF documents from a GitHub repository using a custom function. The downloaded files are stored locally and optionally uploaded into an SQLite database as binary blobs. Subsequently, all PDFs in the directory are read using PyPDFLoader from langchain_community, and their content is split into manageable, overlapping chunks using RecursiveCharacterTextSplitter. This process prepares the textual data for downstream tasks such as embedding or retrieval in a retrieval-augmented generation (RAG) setup.
"""

!mkdir docs

"""Now upload all the files to this directory"""

!pip install wget -q

"""The below code allows the user to import data from a github reporsitory or google drive link, which is optional."""

#Optional chunk to retieve sample files from the github repository.
import wget
import os
import requests

def get_github_files(repo_owner, repo_name, directory_path):
  """
  Fetches a list of PDF files from a GitHub repository directory.

  Args:
      repo_owner (str): The owner of the GitHub repository.
      repo_name (str): The name of the GitHub repository.
      directory_path (str): The path to the directory within the repository.

  Returns:
      list: A list of file URLs for the PDF files in the directory.
  """
  api_url = f"https://api.github.com/repos/{repo_owner}/{repo_name}/contents/{directory_path}"
  headers = {"Accept": "application/vnd.github+json"}  # For the latest API version
  response = requests.get(api_url, headers=headers)
  response.raise_for_status()  # Raise an exception for bad status codes

  pdf_files = []
  for file_data in response.json():
      if file_data["type"] == "file" and file_data["name"].endswith(".pdf"):
          # Use file_data['path'] to construct the correct download URL
          # to handle spaces and special characters in file names.
          download_url = f"https://raw.githubusercontent.com/{repo_owner}/{repo_name}/main/{file_data['path']}"
          pdf_files.append(download_url)
  return pdf_files

# --- Usage ---
repo_owner = "ishanv13"
repo_name = "NHS-Healthcare-Assistant"
directory_path = "Patient2"

pdf_urls = get_github_files(repo_owner, repo_name, directory_path)

# Create the 'docs' directory if it doesn't exist
os.makedirs("docs", exist_ok=True)

# Download the PDF files
for url in pdf_urls:
    filename = os.path.basename(url)
    wget.download(url, out=os.path.join("docs", filename))
    print(f"Downloaded: {filename}")

!pip install pysqlite3

"""The user can upload the initial files or report here."""

'''
import sqlite3
import os

def upload_pdf_to_db(pdf_path, db_name="my_database.db"):
  """
  Uploads PDF content to a SQLite database, storing it in the 'docs' folder.

  Args:
      pdf_path (str): The path to the PDF file.
      db_name (str, optional): The name of the database file. Defaults to "my_database.db".
  """
  with open(pdf_path, "rb") as f:
      pdf_data = f.read()

  conn = sqlite3.connect(db_name)
  cursor = conn.cursor()

  # Create the 'docs' table if it doesn't exist
  cursor.execute("""
      CREATE TABLE IF NOT EXISTS docs (
          id INTEGER PRIMARY KEY AUTOINCREMENT,
          filename TEXT,
          data BLOB
      )
  """)

  # Insert the PDF data into the 'docs' table
  cursor.execute("INSERT INTO docs (filename, data) VALUES (?, ?)", (os.path.basename(pdf_path), pdf_data))

  conn.commit()
  conn.close()

# Example usage within Colab (replace with your file upload method)
from google.colab import files

# Create the directory if it doesn't exist
os.makedirs("/content/docs", exist_ok=True)

uploaded = files.upload()

for filename, data in uploaded.items():
    # Save the file to /content/docs
    file_path = os.path.join("/content/docs", filename)
    with open(file_path, "wb") as f:
        f.write(data)
    upload_pdf_to_db(file_path)  # Use the full path for the database

print("PDF files uploaded to the database (docs table).")
'''

!pip install langchain-community

from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

"""Read all the files in the directory"""

import os
# Assuming PDFs are in a 'docs' folder
pdf_folder_path = 'docs/'
if not os.path.exists(pdf_folder_path):
    print(f"Error: '{pdf_folder_path}' not found. Please upload your PDFs there.")
else:
    loaders = [PyPDFLoader(os.path.join(pdf_folder_path, fn)) for fn in os.listdir(pdf_folder_path) if fn.endswith('.pdf')]
    print(f"Found {len(loaders)} PDF documents.")
    docs = []
    for loader in loaders:
        docs.extend(loader.load())
    print(f"Loaded {len(docs)} pages total.")

"""Split the documents into chunks that are overlapping.

"""

text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=25)
splits = text_splitter.split_documents(docs)
print(f"Split into {len(splits)} chunks.")

for i, split in enumerate(splits):
    print(f"Chunk {i + 1}:\n{split.page_content}\n")

"""##Embedding and Indexing

In this step, we transform the split document chunks into vector embeddings using a pre-trained sentence transformer model (all-MiniLM-L6-v2). These embeddings capture the semantic meaning of the text, enabling efficient similarity-based search. We then store these embeddings in a Chroma vector database, which supports fast retrieval and allows persistence to disk for reuse in future sessions.
"""

from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_community.vectorstores import Chroma

"""We will use a sentence embedding model"""

embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
print("Embedding model loaded.")

"""Create and populate the vector store"""

persist_directory = "db"  # Specify the directory for persistence
# Create or load the Chroma vector store, enabling persistence to disk.
vectorstore = Chroma.from_documents(
   documents=splits, embedding=embeddings, persist_directory=persist_directory)
vectorstore.persist()
print(f"Vector store created and populated with embeddings, persisted to {persist_directory}.")

"""##Gemini LLM Setup

This section integrates Google’s Gemini large language model (LLM) to enable natural language question-answering based on medical records. After setting up the API key and model client, we implement a custom query function that retrieves the most relevant document chunks using semantic similarity, then prompts Gemini to generate two tailored responses: one for patients in layman's terms, and another for doctors using clinical language. Additionally, functionality is provided to dynamically upload new PDFs, reprocess them, and update the vector database accordingly—ensuring the system remains up-to-date and interactive.
"""

!pip install -q -U google-genai  # Install or update google-genai
!pip install -q -U google-generativeai  # Install or update google-generativeai

from google.colab import userdata
from google import genai

# Set your Google API key (ensure it's stored securely)
GOOGLE_API_KEY = userdata.get('Google_API')
client = genai.Client(api_key=GOOGLE_API_KEY)
MODEL = "gemini-2.0-flash"

def answer_with_gemini(query):
    """
    Retrieves semantically similar chunks and uses Gemini to answer the query.

    Args:
        query (str): The user's question.

    Returns:
        str: Gemini's answer to the question.
    """

    # 1. Retrieval of semantically similar chunks:
    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
    retrieved_docs = retriever.invoke(query)

    # 2. Construct the prompt for Gemini:
    context = ""
    for doc in retrieved_docs:
        context += doc.page_content

    # System instructions for summarizing and structuring
    system_instructions = """
    You are a medical AI specialist trained to analyze both structured and unstructured patient health records. Based on the provided {context}, you must generate two distinct outputs:
    Only Give the output in the specified format and don't include the questions in the output:
    \n'Output 1: Patient-Facing Explanation'
    'Answer:
    1.
    2.
    3. '

    'Output 2: Doctor-Facing Summary'
    'Answer:
    1.
    2.
    3. '
    """

    prompt = f"""{system_instructions}

    Output 1: Patient-Facing Explanation

    Use simplified, empathetic language suitable for a non-medical audience. You are a medical assistant explaining to a patient.

    Prompt Format:

    You are a medical assistant explaining to a patient.
    Read the following medical record carefully and then answer the questions below in full sentences.

    {context}

    Questions:
    1. What is the patient’s main health issue?
    2. What do the test results indicate?
    3. {user_question}?

    Answer:

    ⸻

    Output 2: Doctor-Facing Summary

    Use formal medical language appropriate for a professional healthcare provider. Provide evidence-based insights and include clinical rationale when relevant.

    Prompt Format:

    You are a clinical assistant. Read the following patient record:

    {context}

    Now answer the questions in complete sentences, referencing the record:
    1. What is the primary diagnosis?
    2. What are the significant lab or imaging findings?
    3. What treatment plan is recommended, and what guidelines support it?

    Answer:

    """

    # 3. Generate the answer using Gemini:
    response = client.models.generate_content(
        model=MODEL,
        contents=prompt
    )

    return response.text

"""Uploading A new document to the database."""

import os
import sqlite3
from google.colab import files
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_community.vectorstores import Chroma


def update_db_with_pdf(pdf_path, db_name="my_database.db", persist_directory="db"):
    """Updates the Chroma vector database with a new PDF file."""

    # 1. Upload PDF to the database (if not already present).
    try:
        upload_pdf_to_db(pdf_path, db_name)
        print(f"PDF '{pdf_path}' added to the database.")
    except sqlite3.IntegrityError:
        print(f"PDF '{pdf_path}' already exists in the database.")


    # 2. Load and process the new PDF.
    loader = PyPDFLoader(pdf_path)
    new_docs = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=25)
    new_splits = text_splitter.split_documents(new_docs)
    print(f"Loaded and split {len(new_splits)} chunks from the new PDF.")


    # 3. Update the Chroma vector store.
    embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
    vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embeddings)
    vectorstore.add_documents(new_splits)
    vectorstore.persist()
    print("Vectorstore updated with the new PDF content.")


# Example usage (assuming you have a PDF file uploaded to '/content/docs'):

#new_pdf_path = "/content/docs/your_new_pdf.pdf"  # Replace with actual path
#update_db_with_pdf(new_pdf_path)


# --- Helper functions from original code (slightly modified) ---

def upload_pdf_to_db(pdf_path, db_name="my_database.db"):
    """Uploads PDF content to a SQLite database."""
    with open(pdf_path, "rb") as f:
        pdf_data = f.read()
    conn = sqlite3.connect(db_name)
    cursor = conn.cursor()
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS docs (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            filename TEXT UNIQUE,  -- Enforce unique filenames
            data BLOB
        )
    """)
    try:  # Handle potential IntegrityError if the filename exists
        cursor.execute("INSERT INTO docs (filename, data) VALUES (?, ?)", (os.path.basename(pdf_path), pdf_data))
        conn.commit()
    except sqlite3.IntegrityError:
        print(f"File '{os.path.basename(pdf_path)}' already in database. Skipping.")
    conn.close()


# --- Code for file upload and database update ---

uploaded = files.upload()
for filename, data in uploaded.items():
  file_path = os.path.join("/content/docs", filename)
  with open(file_path, "wb") as f:
    f.write(data)
  update_db_with_pdf(file_path)

"""##Medical Assistant

In this final step, the system enables users to ask free-form health-related questions based on the uploaded medical records. By invoking the answer_with_gemini function, it retrieves relevant document context and uses the Gemini LLM to generate a dual-format response tailored for both patients and doctors. This interaction simulates a real-world AI-powered medical assistant capable of delivering personalized insights in real time.
"""

user_question = "Should I go visit a doctor right now?" #@param {type:"string"}
answer = answer_with_gemini(user_question)
print(f"Answer: {answer}")

"""# Use Case and Role-based Output

This RAG-based health assistant serves both patients and doctors by tailoring information to their needs.

For patients, it acts as a friendly translator—turning complex medical data into clear, empathetic explanations, boosting understanding and engagement. For doctors, it’s a smart assistant—surfacing key details from records instantly to save time and reduce oversight.

Uniquely, it delivers two versions of each response: simple for patients, clinical for professionals—making it both practical and trustworthy in real-world care.

# Business Value and Impact

**Saves Clinicians Time:** Quickly summarizes key patient info, reducing time spent on records and improving decision-making.

**Empowers Patients:** Translates medical data into plain language, boosting understanding and treatment adherence.

**Scalable & Modular:** Built with LangChain, ChromaDB, and Gemini for easy deployment across hospitals and EHR systems.

**Competitive Advantage:** Meets growing demand for explainable, AI-driven healthcare—positioning providers as digital leaders.

**Cost-Effective:** Runs serverlessly via platforms like Colab, minimizing infrastructure costs for clinics and startups.

**Reduces GP Wait Times:** Offers safe self-care advice, filtering out non-critical visits and easing pressure on healthcare staff.

# Innovation and Differentiation

Where most assistants stop at giving surface-level responses, this system goes further by being:

**Real-Time and Personalized:** It adapts to new records, instead of depending only on static training data.

**Dual-Purpose:** One system, two outputs — each purpose-built.

**Traceable and Explainable:** Every output can be traced back to the documents it came from, building trust in clinical environments.

These traits turn the assistant into more than a chatbot — it becomes a support layer between data and action.

#Limitations and Future Scope

Few imitations of this model:

**Data Quality:** The assistant works best with clean, well-formatted PDFs. Scanned or handwritten notes can create noise.

**Privacy and Compliance:** Integrating HIPAA or GDPR protocols is essential before real-world deployment.

**Language & Accessibility:** Right now, it works only in English and in text form.


**Future Scope**

We see potential in adding:


*   Voice Input and Output for accessibility
*   Multilingual Support for broader outreach
*   Proactive Alerts that flag early signs of risk using patient history


This project could become a cornerstone of how AI assists in preventive and responsive healthcare.

# Conclusion

At its core, this assistant brings clarity into a system that often overwhelms. By blending retrieval and generation, it becomes a helpful voice that both patients and doctors can rely on — not for replacing them, but for making their lives easier.



It saves time, improves communication, and builds confidence in decision-making. And with every new medical record uploaded, it becomes smarter and more in tune with the individual it serves.



We see it as a step toward a future where healthcare is not just more efficient, but more human — and where AI becomes a trusted part of every patient’s journey.
"""